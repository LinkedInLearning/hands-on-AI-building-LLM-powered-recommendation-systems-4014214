{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "\n",
        "# # Practical Demo: Quantizing a Transformer Model\n",
        "#\n",
        "# In this notebook, we will walk through the process of applying Post-Training Dynamic Quantization to a Hugging Face Transformer model.\n",
        "#\n",
        "# **Our Goal:** To demonstrate that we can make a model significantly **smaller** and **faster** without a major drop in **accuracy**.\n",
        "#\n",
        "# **Our Process:**\n",
        "# 1.  **Establish Baseline:** Load a standard FP32 (32-bit float) model and benchmark its Size, Latency, and Accuracy.\n",
        "# 2.  **Apply Quantization:** Use PyTorch's built-in tools to convert the model to INT8 (8-bit integer).\n",
        "# 3.  **Compare Results:** Benchmark the new INT8 model and compare its performance to the baseline.\n",
        "\n",
        "# ## 1. Setup\n",
        "#\n",
        "# First, we'll install the necessary libraries and import them. We will use `torch` for the model and quantization, `transformers` and `datasets` to get a pre-trained model and evaluation data.\n",
        "\n",
        "# +\n",
        "# !pip install transformers datasets torch -q"
      ],
      "metadata": {
        "id": "-GMCsP7-vLsR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Ensure we're using the CPU for this demo, as dynamic quantization is a CPU optimization\n",
        "device = torch.device(\"cpu\")\n",
        "# -\n",
        "\n",
        "# ## 2. Load Baseline FP32 Model\n",
        "#\n",
        "# We will use `distilbert-base-uncased-finetuned-sst-2-english`, a small but effective model for sentiment analysis. This is our full-precision, 32-bit floating point model.\n",
        "\n",
        "# +\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "fp32_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "fp32_model.to(device)\n",
        "fp32_model.eval() # Set model to evaluation mode\n",
        "\n",
        "print(\"Baseline FP32 model loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "ef4bd0b6607a4ab8af4fae223e6dd744",
            "5e28f912863843d08027f70dbde84105",
            "9eab74279d4f42bd82d38b6026bce39e",
            "c352817bd5774f6d9e3ac19fc609a9c7",
            "c90b5b4a984e4ed4ac4c083360203bae",
            "6f53face35ef46b48b7e2c6d18d57b6c",
            "a1609379830a495da62fb7fc5893d0e2",
            "203bc76298594e598a10effba9f169be",
            "2dea8d5340d742b084f70f510c1aa067",
            "8e2722295fa9484895e5ce38bb92858a",
            "b5ce884bddbb44cea5eeff6c664694f5",
            "de550ecbe8074f64984ecfbff174fe64",
            "9f2dbc40cd2749f498c4cce8b141e930",
            "d13caf7c205e45e5b1c604514effe3d7",
            "74cb2e10ab91442994a10f49ba3edffb",
            "e3a64e2b36d5466f927d02837958e686",
            "e21014ca4f7d4e41aa88b750e8405df3",
            "a650328e06d54b949eff105d415f40c9",
            "6a0777488eef4b1ab321ed3ef6887711",
            "10a4e680ca494e9fa7bb7385bb11f46b",
            "7a64eb9daba14aeaaae931c36513117b",
            "a09b8858ebb24facab851366ec379113",
            "0f30f663929545f2ad59306fc9cddc63",
            "45d335fe75a84dcbb22e1674f1dbec7c",
            "1d7c9377d3394daab0b03fbd6e35f78b",
            "a026a723e63d418cb9786c489531ada9",
            "7c59f21c0525420aaf9374abf31022b2",
            "1dc232efbde04692b09fd4c7c637ff07",
            "5988494f33644942aa7471613b5aa772",
            "1e25c544d6744e63999ab29c3c988a78",
            "c8cfb88a76d24be9955f88b0d85564cb",
            "cf012ac575e34e848a89fcca3ce57392",
            "db9f26dd0ee84572b9a08ff9d04cbfd0",
            "5a40ff64592d48b2a9e39326f4e544de",
            "39ec52fe2b65465188c7b96bd9a793c7",
            "6e3e5270e1504f98a737089b3078396a",
            "81fe57c55ece46529a7cf0a2ac615672",
            "524e2e3ca9de41cd8bb7f3905c9c4fa3",
            "03ad0c860d684245ba7644b5cc1cb131",
            "ec983a54a2bd4c42829104bfdd09ea0c",
            "1b08f01d1bd3479c912c1fceca1e5c59",
            "920ab5f0e065419888bb69b090bf8016",
            "6601dce6a9b2492cba59866937e77eb9",
            "57fdf9786b434ef885f9c48eb74bb9b8"
          ]
        },
        "id": "Y9mJpB6cvLSp",
        "outputId": "e0c5178d-d2fa-490d-c088-2260e0cb146c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef4bd0b6607a4ab8af4fae223e6dd744"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de550ecbe8074f64984ecfbff174fe64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f30f663929545f2ad59306fc9cddc63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a40ff64592d48b2a9e39326f4e544de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline FP32 model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 3. Benchmark the Baseline (FP32) Model\n",
        "#\n",
        "# We'll now measure size, latency, and accuracy.\n",
        "\n",
        "# ### 3.1. Model Size\n",
        "\n",
        "# +\n",
        "# Save the model's state dictionary to get its size on disk\n",
        "torch.save(fp32_model.state_dict(), \"fp32_model.pth\")\n",
        "fp32_size_mb = os.path.getsize(\"fp32_model.pth\") / (1024 * 1024)\n",
        "\n",
        "print(f\"Baseline Model Size (FP32): {fp32_size_mb:.2f} MB\")\n",
        "# -\n",
        "\n",
        "# ### 3.2. Inference Latency\n",
        "# We'll create a helper function to measure the average time it takes to perform inference on a sample sentence. We run it multiple times to get a stable result.\n",
        "\n",
        "# +\n",
        "def measure_latency(model, tokenizer, sentence, num_runs=100):\n",
        "    \"\"\"Measures the average inference latency of a model.\"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Warm-up run\n",
        "    with torch.no_grad():\n",
        "        _ = model(**tokens)\n",
        "\n",
        "    # Timed runs\n",
        "    timings = []\n",
        "    for _ in range(num_runs):\n",
        "        start_time = time.perf_counter()\n",
        "        with torch.no_grad():\n",
        "            _ = model(**tokens)\n",
        "        end_time = time.perf_counter()\n",
        "        timings.append(end_time - start_time)\n",
        "\n",
        "    return np.mean(timings) * 1000 # Return average latency in milliseconds\n",
        "\n",
        "sample_sentence = \"This is a great course, I'm learning a lot!\"\n",
        "fp32_latency_ms = measure_latency(fp32_model, tokenizer, sample_sentence)\n",
        "\n",
        "print(f\"Average Latency (FP32): {fp32_latency_ms:.2f} ms\")\n",
        "# -\n",
        "\n",
        "# ### 3.3. Model Accuracy\n",
        "# Finally, let's check the model's accuracy on the SST-2 validation dataset to ensure it's performing well.\n",
        "\n",
        "# +\n",
        "def evaluate_model(model, tokenizer, dataset):\n",
        "    \"\"\"Evaluates the accuracy of a model on a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        sentence = item['sentence']\n",
        "        label = item['label']\n",
        "\n",
        "        tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokens)\n",
        "            prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "        if prediction == label:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    return (correct / total) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg3bEEOLvMnE",
        "outputId": "db0a8057-5d32-4149-a3a0-dbf52b8722a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model Size (FP32): 255.45 MB\n",
            "Average Latency (FP32): 62.21 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation split of the SST-2 dataset\n",
        "sst2_validation = load_dataset(\"sst2\", split=\"validation\")\n",
        "\n",
        "fp32_accuracy = evaluate_model(fp32_model, tokenizer, sst2_validation)\n",
        "\n",
        "print(f\"Accuracy (FP32): {fp32_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291,
          "referenced_widgets": [
            "e47f38fbbf5a4ec3bafa15360694efd4",
            "8450d907b02f4029977e930d5a9ad88a",
            "498e368d50104ae0bfef569fb28084fb",
            "a3189c5107b54cf1a8e39447fec928d7",
            "d479919e190944f38c0cf949e19bd356",
            "8542beaec6f44477b5b36e1fc5639249",
            "e0f40c47a3974da3b5130e519eb86421",
            "2414400a564e4963b08e5807aa7ae360",
            "e611fd01f91047728143b52aaa2f1aad",
            "d6176976a7fd45d7855e87ce987eafe9",
            "61679ac239ce4766b9e384079f3d8724",
            "cd499221010a4249befff320667c8837",
            "5a63b763487c488db5134a95ecd83e6e",
            "ac35e95289b84f79900b5700891b71ac",
            "8cbe06e60a954790bd68103e56a46559",
            "154c6a28dff34612b54d69e2b7cb3c57",
            "253e72dd7b8f4bf4b6503eb4df595b17",
            "6fd86d128db04e289b0b189aab6efa7b",
            "634e1459bf854bb98f9bd89ffc8e8c55",
            "6503e9371857407cb9047de7c744e713",
            "6a35b3eed2e8453da1e2f0e98d988454",
            "1da5552b589b415eaacdce871dc5bbfc",
            "d4f06d28aa44431fb6c7fdf27918e5fe",
            "e613ae9710f5401cb1de023449221421",
            "7480a3619a9f429088d0827cc831fc93",
            "c2f0a7143f9a4c55a54dcfab8486f8e7",
            "cefb32c3e09b42948bea53376e84d54a",
            "597480f96329452b935258ddb41d5821",
            "588e9be6faa2417caafbaca19dbe9d2d",
            "50315688b8474e9489d4f4ab29eb139e",
            "64343200a83d465b80d17d002f86d36f",
            "da5274753ae04ae9b4aed1eb63891d6d",
            "79d9b919d1fc4799b776050eaa171f28",
            "dd039696079542c5995ec03d2286fdce",
            "e97faf88bfa046848293dfdea2b30a3c",
            "b1216682fdab4996b93483d1c22f7cf8",
            "2315b506205943238a6a985a6f047952",
            "825f4bcdc49046b0b5d5d277cb2027c0",
            "416a63c116514576b23cde79cda4b89f",
            "31c10667a26a40e7ba410747b9e19261",
            "d2525ae9bfea4ffda4a5a842635c82bc",
            "fb24b576854948e480dd40957c8950b1",
            "86190528c8fc465bb36847a07899b8b0",
            "4b86c6c2b1684c94814eb5b9b2f4c46a",
            "0c1d8d59b83840809d129a36197db723",
            "774a97962fba45a2a7d9172c959829eb",
            "c8d500e7802045e1b9837894cc924f66",
            "7790529791164de3a55e72c287ef655c",
            "a5ceba8079ce4a83bbe1c389edae605c",
            "8c009701fd824b20a1e01a6a5a13cce0",
            "725e4e5858cb4b4c9fd6c83b6b08618e",
            "606b2928d8904d37bc72f52bf04265aa",
            "0e21d901069142d0a88b2acdebb2e8a8",
            "c3110b5a6cfc43d29bb2868d863990a8",
            "c2cbaeb8e62040e6904e61a6869fc4ee",
            "b7298b2f95bc43fbb5fc25cc25524b17",
            "80b2e7c42a0f4d18900d4c2102306087",
            "f994b73e254d48858a59007611b994f2",
            "d3045ac754df4bc482fde6a6e7bc38e5",
            "6a5303f9dec24da2af10eee49cbc7f03",
            "a643c20873da4500a80339a6eec56344",
            "9b9bcbd1d90147bab9661c5ce01aab58",
            "b1e99905d04148b2921f2e0dc2c82066",
            "2fe63b3267df40e6b5f891197b14875e",
            "97b029be36a047c8815d2100191ee207",
            "1afa6a963dc54d0190fb127256d3cc93",
            "4f95dbb99195420995226e8936221e3d",
            "92ea987aec814a12895ef9b8eec7c55e",
            "b233eb2ef10c47159382688572f4d667",
            "f7f516e13ef04912add0aec29dbcf820",
            "3bdfc36a829f4dd99a1352acda26b2f9",
            "ada887e7f5c74965b46e054f7575fa11",
            "23afab0a8c26416c84683853f95d3e0e",
            "a769449791a54d6aaff1e3e69912a194",
            "553f72700205417e8f2ea33d479d37bf",
            "f316e110bdc94db49e85c09308850910",
            "da8c663abafc47169a48c5dab7b48284",
            "76ba7222447041f99879edd8ab85bb86",
            "919d041cc05249e798d259fcb50a8595",
            "603c35dd6274408e943e0ea58cdefda2",
            "b44499acc5f446849a18852e6f271758",
            "ca7e21817a9a4d5890d8f06a12dfa218",
            "b7a982d1eba0412fb1818a9478923656",
            "b0310a22cc5944ac8eba45f7e64d51f4",
            "fb2404d421344707a23423fec1251a58",
            "7c195c43b2454504b85fcb1d2faa7c31",
            "4bed185474c24cc9ab90ff15c2b7b105",
            "22e969e40d024157a3fe4683baa657bc"
          ]
        },
        "id": "Lrg3OiV8vRRW",
        "outputId": "cc15595a-2eb6-4a1b-e2c2-1366cf20d226"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e47f38fbbf5a4ec3bafa15360694efd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd499221010a4249befff320667c8837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4f06d28aa44431fb6c7fdf27918e5fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd039696079542c5995ec03d2286fdce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c1d8d59b83840809d129a36197db723"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7298b2f95bc43fbb5fc25cc25524b17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f95dbb99195420995226e8936221e3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/872 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76ba7222447041f99879edd8ab85bb86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (FP32): 91.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 4. Apply Post-Training Dynamic Quantization\n",
        "#\n",
        "# Now for the core step. We use `torch.quantization.quantize_dynamic` to convert our FP32 model to a quantized INT8 model. We specify that we only want to quantize the `Linear` layers, which is a standard practice for Transformer models.\n",
        "\n",
        "# +\n",
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    fp32_model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "print(\"Model has been quantized.\")\n",
        "print(\"\\nOriginal Model Architecture:\\n\", fp32_model)\n",
        "print(\"\\nQuantized Model Architecture:\\n\", quantized_model)\n",
        "# -\n",
        "\n",
        "# ## 5. Benchmark the Quantized (INT8) Model\n",
        "#\n",
        "# Let's run the exact same benchmarks and see the improvements.\n",
        "\n",
        "# ### 5.1. Model Size\n",
        "\n",
        "# +\n",
        "# Save the quantized model's state dictionary\n",
        "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n",
        "quantized_size_mb = os.path.getsize(\"quantized_model.pth\") / (1024 * 1024)\n",
        "\n",
        "print(f\"Quantized Model Size (INT8): {quantized_size_mb:.2f} MB\")\n",
        "print(f\"Size Reduction: {fp32_size_mb / quantized_size_mb:.1f}x smaller\")\n",
        "# -\n",
        "\n",
        "# ### 5.2. Inference Latency\n",
        "\n",
        "# +\n",
        "int8_latency_ms = measure_latency(quantized_model, tokenizer, sample_sentence)\n",
        "\n",
        "print(f\"Average Latency (INT8): {int8_latency_ms:.2f} ms\")\n",
        "print(f\"Speedup: {fp32_latency_ms / int8_latency_ms:.1f}x faster\")\n",
        "# -\n",
        "\n",
        "# ### 5.3. Model Accuracy\n",
        "\n",
        "# +\n",
        "int8_accuracy = evaluate_model(quantized_model, tokenizer, sst2_validation)\n",
        "\n",
        "print(f\"Accuracy (INT8): {int8_accuracy:.2f}%\")\n",
        "print(f\"Accuracy Drop: {fp32_accuracy - int8_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e796a5eaf90c47d6aaa192d700b880d4",
            "aa0e0114d812406fac9d3db898c27a27",
            "18433f88243f49a292d9abf563c61560",
            "ac2671078e5641c0a5c8b0cf3c7ed89c",
            "79de291857f14805b72f2053e1c71a84",
            "18c2ef47f2464923a01fa339d4326bd4",
            "a07a696f1371403da0967c94fc45fe92",
            "e09dacc95dd54614b2e9ae1fca7eb9ae",
            "8f57bbf9bbb84970835575732b438eb1",
            "687c604874944db9bcedc0a9d92b10a8",
            "0ce9db99a9dd4c3c917aaa231b3245c2"
          ]
        },
        "id": "lCf0AE6gvRI8",
        "outputId": "9d1406e8-bd51-4cbc-93f7-9de0c1f7eab8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1563471062.py:7: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been quantized.\n",
            "\n",
            "Original Model Architecture:\n",
            " DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "\n",
            "Quantized Model Architecture:\n",
            " DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Quantized Model Size (INT8): 132.29 MB\n",
            "Size Reduction: 1.9x smaller\n",
            "Average Latency (INT8): 35.36 ms\n",
            "Speedup: 1.8x faster\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/872 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e796a5eaf90c47d6aaa192d700b880d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (INT8): 89.79%\n",
            "Accuracy Drop: 1.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 6. Final Results Comparison\n",
        "#\n",
        "# Let's put all our measurements into a single table to see the final results.\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "summary = f\"\"\"\n",
        "| Metric          | Baseline (FP32)   | Quantized (INT8)  | Improvement      |\n",
        "|-----------------|-------------------|-------------------|------------------|\n",
        "| **Model Size**  | {fp32_size_mb:.2f} MB      | **{quantized_size_mb:.2f} MB**     | **{fp32_size_mb/quantized_size_mb:.1f}x smaller** |\n",
        "| **Latency**     | {fp32_latency_ms:.2f} ms      | **{int8_latency_ms:.2f} ms**      | **{fp32_latency_ms/int8_latency_ms:.1f}x faster** |\n",
        "| **Accuracy**    | {fp32_accuracy:.2f}%       | **{int8_accuracy:.2f}%**      | **-{fp32_accuracy - int8_accuracy:.2f}%**         |\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "IxefmRkzvMap",
        "outputId": "00287a3b-67ca-4de1-d4f1-c1534f4937ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n| Metric          | Baseline (FP32)   | Quantized (INT8)  | Improvement      |\n|-----------------|-------------------|-------------------|------------------|\n| **Model Size**  | 255.45 MB      | **132.29 MB**     | **1.9x smaller** |\n| **Latency**     | 62.21 ms      | **35.36 ms**      | **1.8x faster** |\n| **Accuracy**    | 91.06%       | **89.79%**      | **-1.26%**         |\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0Lj0-ob-HY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}